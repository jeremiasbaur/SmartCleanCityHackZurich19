{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Tweets from Relevant Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweetscrape.profile_tweets import TweetScrapperProfile \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tilia/HZ/SmartCleanCityHackZurich19/data\n"
     ]
    }
   ],
   "source": [
    "%cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180_days_maxT_24hrPrecip.csv\r\n",
      "180_days_mean1hrT_1hrPrecip.csv\r\n",
      "180_days_meanT_24hrPrecip.csv\r\n",
      "2019-09-27-basel-collections.csv\r\n",
      "2019-09-27-basel-image-metadata.csv\r\n",
      "2019-09-27-basel-measures-FEAT-TempPrecip.csv\r\n",
      "2019-09-27-basel-measures-FEAT.csv\r\n",
      "2019-09-27-basel-measures-cleaned.csv\r\n",
      "2019-09-27-basel-measures-prediction-cleaned.csv\r\n",
      "2019-09-27-basel-measures-prediction.csv\r\n",
      "2019-09-27-basel-measures.csv\r\n",
      "all_tweets.csv\r\n",
      "all_tweets_relcols.csv\r\n",
      "event_cal.csv\r\n",
      "twitter.csv\r\n",
      "twitter_accounts.txt\r\n",
      "\u001b[34mtwitter_data\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baselcommunity', 'BaselStadt', 'baseltourism', 'BVB_Leitstelle', 'jsdBS']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('twitter_accounts.txt', 'r') as ta_f:\n",
    "    profiles = [i.rstrip('\\n') for i in ta_f.readlines()]\n",
    "\n",
    "profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download last n tweets from relevant profiles (CAREFUL, this overwrites the current file!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 501 tweets till 2018-05-28 at twitter_data/baselcommunity-500_tweets2.csv for user baselcommunity\n",
      "Extracted 500 tweets till 2019-01-23 at twitter_data/BaselStadt-500_tweets2.csv for user BaselStadt\n",
      "Extracted 500 tweets till 2018-09-19 at twitter_data/baseltourism-500_tweets2.csv for user baseltourism\n",
      "Extracted 500 tweets till 2019-04-01 at twitter_data/BVB_Leitstelle-500_tweets2.csv for user BVB_Leitstelle\n",
      "Extracted 500 tweets till 2016-04-05 at twitter_data/jsdBS-500_tweets2.csv for user jsdBS\n"
     ]
    }
   ],
   "source": [
    "def get_tweets(user_profile, tnum):\n",
    "    '''\n",
    "    Downloads tnum last tweets for a given user profile and saves download as csv file. \n",
    "    Expects a directory 'twitter_data' in the current working directory\n",
    "    '''\n",
    "    \n",
    "    dump_path = f'twitter_data/{user_profile}-{tnum}_tweets.csv'\n",
    "\n",
    "    tweet_scrapper = TweetScrapperProfile(user_profile, tnum, dump_path, 'csv')\n",
    "    tweet_count, tweet_id, tweet_time, dump_path = tweet_scrapper.get_profile_tweets()\n",
    "    print(\"Extracted {0} tweets till {1} at {2} for user {3}\".format(tweet_count, tweet_time, dump_path, user_profile))\n",
    "    \n",
    "    \n",
    "for p in profiles:\n",
    "    \n",
    "    get_tweets(p, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add date information in default formatting and join all dataframes into a global dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daydate(date_str):\n",
    "    '''Returns shortened date string in the format %Y-%m-%d.'''\n",
    "    \n",
    "    d = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "       \n",
    "    return str(datetime.datetime.strftime(d, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.DataFrame()\n",
    "\n",
    "for user_profile in profiles:\n",
    "\n",
    "    tw = pd.read_csv(f'twitter_data/{user_profile}.csv')\n",
    "    tw['date'] = tw.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)/1000))\n",
    "    tw['day_date'] = tw.date.apply(lambda x: get_daydate(str(x)))\n",
    "    all_tweets = all_tweets.append(tw)\n",
    "      \n",
    "all_tweets.to_csv('twitter_data/all_tweets.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out another csv file which only contains columns corresponding to (possibly) relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_relcols = all_tweets[['id', 'author', 're_tweeter', 'text', 'hashtags', 'mentions', 'favorite_count', 'day_date']].copy()\n",
    "\n",
    "all_tweets_relcols.to_csv('twitter_data/all_tweets_relcols.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartenv",
   "language": "python",
   "name": "smartenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
